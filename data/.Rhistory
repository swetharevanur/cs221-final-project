panel.grid.minor = element_blank(),
plot.background = element_rect(fill = "#f5f5f2", color = NA),
panel.background = element_rect(fill = "#f5f5f2", color = NA),
legend.background = element_rect(fill = "#f5f5f2", color = NA),
panel.border = element_blank(),
...
)
}
# Read in input data
data <- read.csv("Documents/dataviz/avg_age_15.csv", stringsAsFactors = F)
# Read in geodata
gde_15 <- readOGR("Documents/dataviz/gde-1-1-15.shp", layer = "gde-1-1-15")
## OGR data source with driver: ESRI Shapefile
## Source: "input/geodata/gde-1-1-15.shp", layer: "gde-1-1-15"
## with 2324 features
## It has 2 fields
# set crs to ch1903/lv03, just to make sure  (EPSG:21781)
crs(gde_15) <- "+proj=somerc +lat_0=46.95240555555556
+lon_0=7.439583333333333 +k_0=1 +x_0=600000 +y_0=200000
+ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs"
# fortify, i.e., make ggplot2-compatible
map_data_fortified <- fortify(gde_15, region = "BFS_ID") %>%
mutate(id = as.numeric(id))
# now we join the thematic data
map_data <- map_data_fortified %>% left_join(data, by = c("id" = "bfs_id"))
# read in background relief
relief <- raster("Documents/dataviz/02-relief-georef-clipped-resampled.tif")
relief_spdf <- as(relief, "SpatialPixelsDataFrame")
# relief is converted to a very simple data frame,
# just as the fortified municipalities.
# for that we need to convert it to a
# SpatialPixelsDataFrame first, and then extract its contents
# using as.data.frame
relief <- as.data.frame(relief_spdf) %>%
rename(value = `X02.relief.georef.clipped.resampled`)
# remove unnecessary variables
rm(relief_spdf)
rm(gde_15)
rm(map_data_fortified)
p <- ggplot() +
# municipality polygons
geom_polygon(data = map_data, aes(fill = avg_age_15,
x = long,
y = lat,
group = group)) +
# municipality outline
geom_path(data = map_data, aes(x = long,
y = lat,
group = group),
color = "white", size = 0.1) +
coord_equal() +
# add the previously defined basic theme
theme_map() +
labs(x = NULL,
y = NULL,
title = "Switzerland's regional demographics",
subtitle = "Average age in Swiss municipalities, 2015",
caption = "Geometries: ThemaKart, BFS; Data: BFS, 2016")
p
path = "/Users/swetharevanur/Documents/2_Sophomore/1_Fall/CS 221/cs221-final-project/data/"
setwd(path)
rm(df)
fileNames = unlist(list.files(pattern = "second_total_list_working*.xlsx"))
print(fileNames)
# fileName = "second_total_file_working*.xlsx"
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read.xls(filePath,sheet = 1,header = TRUE)
df = rbind(filedf)
}
View(df)
fileNames = unlist(list.files(pattern = "second_total_list_working*.xlsx"))
fileNames
fileNames = unlist(list.files(pattern = "second_total_file_working*.xlsx"))
fileNames
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
View(df)
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE, index = FALSE)
colnames(df) = columnNames
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
View(df)
fileName
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read.xls(filePath,sheet = 1,header = FALSE)
df = rbind(filedf)
}
library(topicmodels)
library(doParallel)
library(ggplot2)
library(scales)
library(tidyverse)
library(RColorBrewer)
library(wordcloud)
library(ldatuning)
library(gdata)
library(tm)
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read.xls(filePath,sheet = 1,header = FALSE)
df = rbind(filedf)
}
View(df)
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read.xls(filePath, sheet = 1, header = TRUE)
df = rbind(filedf)
}
View(df)
fileNames = unlist(list.files(pattern = "second_total_file_working.xlsx"))
print(fileNames)
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read.xls(filePath, sheet = 1, header = TRUE)
df = rbind(filedf)
}
View(df)
install.packages("readxl")
install.packages("readxl")
install.packages("readxl")
library(readxl)
filePath
filedf = read.excel(filePath, skip = 1)
filedf = read_excel(filePath, skip = 1)
View(filedf)
filedf = read_excel(filePath)
View(filedf)
View(filedf)
dim(filedf)
path = "/Users/swetharevanur/Documents/2_Sophomore/1_Fall/CS 221/cs221-final-project/data/"
setwd(path)
fileNames = unlist(list.files(pattern = "filtered_third_total_file_list.xlsx"))
print(fileNames)
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read_excel(filePath)
# filedf = read.xls(filePath, sheet = 1, header = TRUE)
df = rbind(filedf)
}
View(df)
path = "/Users/swetharevanur/Documents/2_Sophomore/1_Fall/CS 221/cs221-final-project/data/"
setwd(path)
fileNames = unlist(list.files(pattern = "filtered_third_total_file_list.xlsx"))
print(fileNames)
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read_excel(filePath)
# filedf = read.xls(filePath, sheet = 1, header = TRUE)
df = rbind(filedf)
}
# add another column that is the concatenation of the title + text columns
colsToPaste = c('postText', 'postTitle')
# create a new column `x` with the three columns collapsed together
df$mergedPost <- apply(df[, colsToPaste], 1, paste, collapse = " ")
corpus<-Corpus(VectorSource(c(df['mergedPost'])))
full_data = DocumentTermMatrix(corpus)
system.time({
tunes <- FindTopicsNumber(
full_data,
topics = c(1:10 * 10, 120, 140, 160, 180, 0:3 * 50 + 200),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 4L,
verbose = TRUE
)
})
FindTopicsNumber_plot(tunes)
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 200, 300) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
control = list(burnin = burnin, iter = iter, keep = keep) )
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs"))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
burnin = 1000
iter = 1000
keep = 50
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep)))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
burnin = 1000
iter = 1000
keep = 50
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)
ggplot(results_df, aes(x = k, y = perplexity)) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("5-fold cross-validation of topic modelling with the 'Associated Press' dataset",
"(ie five different models fit for each candidate number of topics)") +
labs(x = "Candidate number of topics", y = "Perplexity when fitting the trained model to the hold-out set")
burnin = 1000
iter = 1000
keep = 50
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
View(results)
burnin = 1000
iter = 1000
keep = 50
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
library(topicmodels)
})
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 300) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))
# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
k <- candidate_k[j]
results_1k <- matrix(0, nrow = folds, ncol = 2)
colnames(results_1k) <- c("k", "perplexity")
for(i in 1:folds){
train_set <- full_data[splitfolds != i , ]
valid_set <- full_data[splitfolds == i, ]
fitted <- LDA(train_set, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))
results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
}
return(results_1k)
}
})
stopCluster(cluster)
View(results)
path = "/Users/swetharevanur/Documents/2_Sophomore/1_Fall/CS 221/cs221-final-project/data/"
setwd(path)
fileNames = unlist(list.files(pattern = "filtered_third_total_file_list.xlsx"))
print(fileNames)
# declare empty dataframe
columnNames = c("postCategory", "postDay", "postDistrict", "postID",
"postLocation", "postMonth", "postOtherAds",
"postPhone", "postText", "postTime", "postTitle", "postYear")
df <- data.frame(matrix(ncol = length(columnNames), nrow = 0), stringsAsFactors = FALSE)
colnames(df) = columnNames
# populate df
for (fileName in fileNames) {
fileName = toString(fileName)
filePath = paste(path, fileName, sep = "")
filedf = read_excel(filePath)
# filedf = read.xls(filePath, sheet = 1, header = TRUE)
df = rbind(filedf)
}
# add another column that is the concatenation of the title + text columns
colsToPaste = c('postText', 'postTitle')
# create a new column `x` with the three columns collapsed together
df$mergedPost <- apply(df[, colsToPaste], 1, paste, collapse = " ")
corpus<-Corpus(VectorSource(c(df['mergedPost'])))
full_data = DocumentTermMatrix(corpus)
system.time({
tunes <- FindTopicsNumber(
full_data,
topics = c(2, 3, 4, 5, 6, 7, 8, 9, 1:10 * 10, 120, 140, 160, 180, 0:3 * 50 + 200),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 4L,
verbose = TRUE
)
})
FindTopicsNumber_plot(tunes)
FindTopicsNumber_plot(tunes[0:17])
View(tunes)
FindTopicsNumber_plot(tunes[1:15,])
FindTopicsNumber_plot(tunes[1:20,])
FindTopicsNumber_plot(tunes[c(1:18, 23),])
FindTopicsNumber_plot(tunes[1:18,])
